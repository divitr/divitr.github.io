<!DOCTYPE html>
<html lang="en">

<head>
    <div style='display:none'>\(\require{physics}\)</div>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="../../style.css">
    <link rel="stylesheet" href="../posts.css">
    <script src="../posts_header.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Learning Training Dynamics are not Fisher Geodesics</title>

    <!-- Configure MathJax to wait for TOC/footnotes before rendering -->
    <script>
        window.MathJax = {
            startup: {
                pageReady: () => {
                    // Wait for TOC and footnotes to initialize first
                    return new Promise((resolve) => {
                        if (document.readyState === 'loading') {
                            document.addEventListener('DOMContentLoaded', () => {
                                // Give TOC/footnotes a moment to set up
                                setTimeout(resolve, 100);
                            });
                        } else {
                            setTimeout(resolve, 100);
                        }
                    }).then(() => MathJax.startup.defaultPageReady());
                }
            }
        };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
    <div id="header-placeholder"></div>

    <main class="blog-post">
        <section class="intro">
            <h1>Deep Learning Training Dynamics are not Fisher Geodesics</h1>
            <div class="post-tags"><span class="tag" data-tag="math">math</span><span class="tag" data-tag="physics">physics</span><span class="tag" data-tag="ml">ml</span></div>
            <p class="date">Nov 16, 2025</p>
            <p class="desc">A cool bit of math that falls apart empirically</p>
        </section>

<p><strong>[NOTE]</strong> This post is mostly just because the idea was pretty cool (imo) and once the empirics failed, I wanted somewhere to keep it. Most of this post is transcribed from the original LATEX document by GPT, and as such, may have some innaccuracies or syntax errors.</p>
<h2>Introduction</h2>
<p>Different optimizers like SGD and Adam often produce similar loss curves and generalization behavior, even though they use different update rules. SGD does Euclidean gradient descent, Adam uses adaptive diagonal preconditioning, and natural gradient uses the Fisher information matrix. So why do they end up with similar training dynamics?</p>
<p>Over the past couple months I worked on a theoretical framework that training paths are approximately geodesics in Fisher information space<sup><a href="#fn1" id="fnref1" aria-label="Footnote 1">1</a></sup>, and that different optimizers' metrics conformally align at large width, causing their paths to coincide. The theory has three main components: (i) all preconditioned gradient descent methods are equivalent to Riemannian gradient flow under different metrics, (ii) stochastic gradient noise concentrates paths near geodesics via large deviations, and (iii) in overparameterized networks, different optimizers' metrics align with the Fisher metric at rate \(O(p^{-1/2})\).</p>
<p>This would explain why neural scaling laws work, why hyperparameter transfer schemes like \(\mu\)P succeed, and why training is robust across optimizer choices. However, this theory doesn't hold empirically (due to problems with misspecified models and short time limits).</p>
<h2>Preliminaries</h2>
<p>Before diving into the theory, we need some background concepts from differential geometry, information geometry, and stochastic analysis. This section provides the necessary mathematical foundation.</p>
<h3>Information Geometry and Statistical Manifolds</h3>
<p>Information geometry studies probability distributions as geometric objects. A parametric family of probability distributions \(\{p_\theta(y|x)\}_{\theta \in \Theta}\) forms a statistical manifold, where each point \(\theta \in \Theta\) represents a different distribution. This lets us measure distances between distributions, define geodesics (the "straight lines" of the manifold), and understand how optimization moves through the space of models.</p>
<p>The space of probability distributions has geometric structure that doesn't depend on parameterization. The Fisher information metric captures this structure and provides a way to measure distances and angles.</p>
<h3>Riemannian Manifolds and Metrics</h3>
<p>A Riemannian manifold \((\Theta, g)\) consists of a smooth manifold \(\Theta\) equipped with a metric tensor \(g\): a smoothly varying inner product \(g(\theta): T_\theta\Theta \times T_\theta\Theta \to \mathbb{R}\) on each tangent space. The tangent space \(T_\theta\Theta\) at a point \(\theta\) consists of all possible directions in which we can move from \(\theta\); in the context of neural networks, these are directions of parameter change.</p>
<p>In coordinates, if \(\theta \in \mathbb{R}^p\), the metric is a symmetric positive definite matrix \(g(\theta) \in \mathbb{R}^{p \times p}\), defining the inner product \(\langle u, v \rangle_{g(\theta)} = u^\top g(\theta) v\) and inducing the norm \(\norm{v}_{g(\theta)} = \sqrt{v^\top g(\theta) v}\) for tangent vectors \(u, v \in T_\theta\Theta \cong \mathbb{R}^p\). The metric tensor tells us how to measure lengths and angles in the tangent space, which varies smoothly as we move through the manifold.</p>
<p>Different metrics give different notions of distance. The Euclidean metric \(g = I\) treats all parameter directions equally, while the Fisher metric weights directions by how much they affect the model's predictions.</p>
<h3>Geodesics</h3>
<p>A geodesic is a curve \(\gamma: [0,T] \to \Theta\) that locally minimizes distance. In Euclidean space, geodesics are straight lines. On a curved manifold, geodesics are curves with zero acceleration in the Riemannian sense.</p>
<p>Geodesics satisfy the geodesic equation \(\frac{\mathrm D}{\mathrm dt}\dot{\gamma}(t) = 0\), where \(\frac{D}{dt}\) is the Levi-Civita covariant derivative<sup><a href="#fn2" id="fnref2" aria-label="Footnote 2">2</a></sup> of \(g\). This means the velocity vector \(\dot{\gamma}\) is parallel-transported along the curve—it doesn't rotate relative to the local geometry.</p>
<p>A minimizing geodesic between two points is the shortest path connecting them; in a normal convex neighborhood, such geodesics are unique. The length of a curve \(\gamma: [0,T] \to \Theta\) is given by \(\int_0^T \norm{\dot{\gamma}(t)}_{g(\gamma(t))} \mathrm{d}t\), and geodesics minimize this length among all curves connecting their endpoints.</p>
<h3>Fisher Information Metric</h3>
<p>For a parametric family of probability distributions \(\{p_\theta(y|x)\}_{\theta \in \Theta}\) with log-likelihood \(\ell(\theta; x, y) = \log p_\theta(y|x)\) and score function \(s(\theta; x, y) = \nabla_\theta \ell(\theta; x, y)\), the Fisher information matrix is \begin{equation} \mathcal{I}(\theta) = \mathbb{E}_{(x,y) \sim \mathcal{D}}\left[s(\theta; x, y) s(\theta; x, y)^\top\right], \end{equation} where \(\mathcal{D}\) is the data distribution and the expectation is taken under the model distribution \(y \sim p_\theta(\cdot|x)\). The score function measures how sensitive the log-likelihood is to parameter changes, and the Fisher information captures the expected squared sensitivity.</p>
<p>The Fisher information defines a Riemannian metric on the statistical manifold \(\Theta\), known as the Fisher-Rao metric. This metric is uniquely characterized (up to scaling) by its invariance under sufficient statistics and reparameterizations. The Fisher metric has several important properties:</p>
<ul><li>It measures distances between probability distributions in a way that's invariant to how we parameterize the model</li><li>It weights parameter directions by their impact on the model's predictions</li><li>It provides a natural geometry for understanding optimization in the space of models</li><li>In the context of neural networks, it captures how sensitive the network's output is to changes in each parameter</li></ul>
<p>Under regularity conditions, the Fisher information also equals the negative expected Hessian: \(\mathcal{I}(\theta) = -\mathbb{E}\left[\nabla_\theta^2 \ell(\theta; x, y)\right]\), connecting it to the curvature of the log-likelihood surface.</p>
<h3>Natural Gradient Descent</h3>
<p>Gradient descent in the Fisher metric is called natural gradient descent. The natural gradient is defined as \(\nabla^{\mathcal{I}} L(\theta) = \mathcal{I}(\theta)^{-1} \nabla L(\theta)\), where \(\nabla L\) is the standard Euclidean gradient. Natural gradient descent follows the update rule: \begin{equation} \theta_{k+1} = \theta_k - \eta \mathcal{I}(\theta_k)^{-1} \nabla L(\theta_k). \end{equation}</p>
<p>Natural gradient descent has several appealing properties: it's invariant to reparameterizations of the model, it accounts for the geometry of the parameter space, and it can converge faster than standard gradient descent in certain settings. However, computing and inverting the full Fisher information matrix is computationally expensive for large neural networks, which is why practical optimizers use approximations.</p>
<h3>Conformal Equivalence</h3>
<p>Two metrics \(g\) and \(\tilde{g}\) on \(\Theta\) are conformally equivalent if \(\tilde{g}(\theta) = c(\theta) g(\theta)\) for some positive scalar field \(c: \Theta \to \mathbb{R}_{>0}\). Conformally equivalent metrics have the same geodesics (as unparameterized curves), differing only in how they are traversed in time.</p>
<p>Specifically, if \(\gamma(t)\) is a geodesic of \(g\), then \(\gamma(\tau(t))\) is a geodesic of \(\tilde{g}\) where \(\tau\) satisfies \(\mathrm d\tau/\mathrm dt = c(\gamma(t))\). This time reparameterization property is central to our analysis: if two optimizers' metrics are conformally equivalent, they follow the same paths through parameter space, just at different speeds.</p>
<p>Conformal equivalence is weaker than metric equality but stronger than just having similar geodesics. It means the metrics differ only by a scalar factor at each point, preserving angles and the shape of geodesics while allowing different traversal speeds.</p>
<h3>Large Deviations Theory and Stochastic Processes</h3>
<p>Large deviations theory<sup><a href="#fn3" id="fnref3" aria-label="Footnote 3">3</a></sup> studies the asymptotic behavior of rare events. For our purposes, we use it to show that stochastic processes concentrate on certain "most probable" paths. The Onsager-Machlup functional provides a rate function that characterizes the probability of paths deviating from the most probable one.</p>
<p>When we add noise to gradient descent (as in mini-batch SGD), the resulting stochastic process can be analyzed using large deviations theory. The theory tells us that, as the noise strength goes to zero, the process concentrates on paths that minimize a certain "action" functional. For diffusions on Riemannian manifolds, these most probable paths are often geodesics.</p>
<p>This suggests that noisy optimization might follow geodesic paths in the limit of small noise. However, this breaks down for long time horizons and misspecified models.</p>
<h2>The Theoretical Framework</h2>
<p>The theory rests on three pillars:</p>
<ol type="a"><li><strong>Optimizers as Riemannian gradient flows</strong>: All preconditioned gradient descent methods are equivalent to Riemannian gradient flow under different metric tensors.</li><li><strong>Stochastic dynamics concentrate near geodesics</strong>: Mini-batch gradient noise is Fisher-covariant, and stochastic bridge processes concentrate on geodesics via large deviations.</li><li><strong>Metric alignment in overparameterized networks</strong>: In the NTK regime, different optimizers' metrics conformally align with the Fisher metric at rate \(O(p^{-1/2})\).</li></ol>
<p>Let's develop each of these in detail.</p>
<h2>Optimizers as Riemannian Gradient Flows</h2>
<p>The first part of the theory shows that preconditioned gradient descent methods are all instances of Riemannian gradient descent under different metrics. This lets us compare optimizers by comparing their metrics.</p>
<h3>Setup</h3>
<p>Consider a loss function \(L: \Theta \subset \mathbb{R}^p \to \mathbb{R}\) and a preconditioner \(P: \Theta \to \mathbb{R}^{p \times p}\) that is symmetric positive definite at each point. The preconditioner \(P\) modifies the gradient before applying the update. Define the metric tensor \(G(\theta) = P(\theta)^{-1}\). This inverse relationship is key: the preconditioner and the metric are dual to each other.</p>
<h3>The Main Result</h3>
<div class="example-box">
<div class="example-box-title">Preconditioned Gradient Descent is Riemannian Steepest Descent</div>
<div class="example-box-prompt">
Let \(L: \Theta \subset \mathbb{R}^p \to \mathbb{R}\) be \(C^1\), and let \(P: \Theta \to \mathbb{R}^{p \times p}\) be continuous with \(P(\theta)\) symmetric positive definite for all \(\theta\). Define the metric \(G(\theta) = P(\theta)^{-1}\). Consider the preconditioned gradient descent update
\begin{equation}
    \theta_{k+1} = \theta_k - \eta P(\theta_k) \nabla L(\theta_k).
\end{equation}
Then:
<ol type="a"><li>This update performs Riemannian gradient descent with respect to metric \(G\).</li><li>The continuous-time limit as \(\eta \to 0\) is the Riemannian gradient flow \(\dot{\theta}(t) = -P(\theta(t))\nabla L(\theta(t)) = -\nabla^G L(\theta(t))\).</li><li>If \(P(\theta) = c(\theta) g(\theta)^{-1}\) for scalar \(c(\theta) > 0\) and metric \(g\), then trajectories coincide with \(g\)-gradient flow up to time reparameterization \(\mathrm{d}\tau = c(\theta(t))\mathrm{d}t\).</li></ol>
</div>
</div>
<h3>Proof</h3>
<p>The key insight is that preconditioned gradient descent minimizes a local first-order approximation subject to a trust region constraint in the \(G\)-metric.</p>
<h4>Part 1: One-Step Minimizer</h4>
<p>The update \(\theta^+ = \theta + \Delta^{\ast}\) where \(\Delta^{\ast}(\theta) = \arg \min_{\Delta \in \mathbb{R}^p} \left\{\langle \nabla L(\theta), \Delta\rangle + \frac{1}{2\eta} \norm{\Delta}^2_{G(\theta)}\right\}\).</p>
<p>The objective is strictly convex since \(G(\theta) \succ 0\). Taking the gradient with respect to \(\Delta\): \begin{equation} \nabla_\Delta \left(\langle\nabla L(\theta), \Delta\rangle + \frac{1}{2\eta} \Delta^\top G(\theta) \Delta\right) = \nabla L(\theta) + \frac{1}{\eta}G(\theta)\Delta. \end{equation}</p>
<p>Setting to zero: \(G(\theta)\Delta^{\ast} = -\eta\nabla L(\theta)\). Multiplying by \(G(\theta)^{-1} = P(\theta)\) gives \(\Delta^{\ast} = -\eta P(\theta)\nabla L(\theta)\). Uniqueness follows from strict convexity.</p>
<h4>Part 2: Continuous-Time Limit</h4>
<p>Define the piecewise-linear interpolation \(\theta_{\eta}(t)\) with \(\theta_\eta(k\eta) = \theta_k\) and slope \(\frac{\theta_{k+1} - \theta_k}{\eta} = -P(\theta_k) \nabla L(\theta_k)\) on \([k\eta, (k+1)\eta)\). Under local Lipschitz assumptions on \(P\) and \(\nabla L\), the ODE method for deterministic numerical schemes implies \(\theta_\eta \to \theta\) uniformly on compact time intervals, where \(\theta\) solves \(\dot{\theta} = -P(\theta) \nabla L(\theta)\).</p>
<p>To identify this as Riemannian gradient flow, recall the Riemannian gradient \(\nabla^{G}L\) is the unique vector field satisfying \begin{equation} \langle \nabla^G L(\theta), v \rangle_{G(\theta)} = \mathrm{d}L(\theta)[v] = \langle\nabla L(\theta), v\rangle \quad \text{for all } v. \end{equation}</p>
<p>Thus \(G(\theta) \nabla^{G} L(\theta) = \nabla L(\theta)\), so \(\nabla^{G} L(\theta) = G(\theta)^{-1}\nabla L(\theta) = P(\theta) \nabla L(\theta)\). Therefore the ODE is \(\dot{\theta} = - \nabla^{G} L(\theta)\), which is steepest descent under \(G\).</p>
<h4>Part 3: Conformal Equivalence</h4>
<p>If \(P = cg^{-1}\) with scalar \(c(\theta) > 0\), then \(\dot{\theta} = -c(\theta)g^{-1}\nabla L(\theta)\). Let \(\tau\) satisfy \(\mathrm{d}\tau = c(\theta(t))\mathrm{d}t\). Then \(\frac{\mathrm{d}\theta}{\mathrm{d}\tau} = -g^{-1}\nabla L(\theta)\), so the curves (images in \(\Theta\)) match natural gradient descent, only their speeds differ.</p>
<h3>Examples</h3>
<ul><li><strong>SGD</strong>: \(P = I\) gives \(G = I\), so SGD performs Euclidean gradient flow.</li><li><strong>Adam</strong>: \(P = \mathrm{diag}(v_t)^{-1/2}\) where \(v_t\) tracks coordinate-wise gradient variances, so Adam performs gradient flow under a diagonal adaptive metric.</li><li><strong>Natural Gradient</strong>: \(P = \mathcal{I}(\theta)^{-1}\) gives \(G = \mathcal{I}(\theta)\), the Fisher metric.</li></ul>
<p>If two optimizers' metrics are approximately conformal, \(P_1(\theta) \approx c(\theta) P_2(\theta)\), their training trajectories will approximately coincide as curves in parameter space, differing only in how fast they're traversed.</p>
<h2>Stochastic Dynamics and Geodesic Concentration</h2>
<p>The second part asks: why would noisy optimization follow smooth geometric paths? Large deviations theory shows that stochastic processes concentrate on "most probable" paths, and for diffusions on Riemannian manifolds, these are geodesics.</p>
<h3>Mini-Batch Gradient Noise is Fisher-Covariant</h3>
<p>Gradient noise from mini-batches has a covariance structure that matches the Fisher information matrix. This follows from the structure of likelihood-based learning, not an assumption.</p>
<div class="example-box">
<div class="example-box-title">Mini-Batch Gradient Noise is Fisher-Covariant</div>
<div class="example-box-prompt">
Let \(Z = (X,Y)\) be a training example from data distribution \(\mathsf{P}\), and consider a parametric conditional model \(p_\theta(y \vert x)\) with log likelihood \(\ell(\theta;Z) = \log p_\theta(Y \vert X)\) and score \(s(\theta; Z) = \nabla_\theta \ell(\theta;Z)\).

For mini-batch SGD with batch size \(b\), let \(\hat{g}_b(\theta) = \frac{1}{b} \sum_{i=1}^b s(\theta; Z_i)\) be the mini-batch gradient estimator, and \(g(\theta) = \mathbb{E}_\mathsf{P} \left[-s(\theta; Z)\right]\) the full-batch gradient. Write the gradient noise as \(\xi_b(\theta) = \hat{g}_b(\theta) - g(\theta)\).

<strong>With Replacement</strong>: If \(Z_1, \dots, Z_b\) are i.i.d. from \(\mathsf{P}\), then
\begin{equation}
    \operatorname{Cov}(\xi_b(\theta)) = \frac{1}{b} \operatorname{Cov}_\mathsf{P}(s(\theta; Z)).
\end{equation}

<strong>Well-Specified Case</strong>: If, in addition, the model is well-specified at \(\theta\) (i.e., \(Y \vert X \sim p_\theta(\cdot \vert X)\) under \(\mathsf P\)), then \(\mathbb{E}[s(\theta; Z)] = 0\) and
\begin{equation}
    \operatorname{Cov}(\xi_b(\theta)) = \frac{1}{b} \mathbb{E}[s(\theta; Z)s(\theta; Z)^\top] = \frac{1}{b} \mathcal{I}(\theta),
\end{equation}
where \(\mathcal{I}(\theta)\) is the Fisher information matrix.
</div>
</div>
<h3>Proof</h3>
<p>Since \(\hat{g}_b(\theta) = -(1/b)\sum_{i=1}^b s(\theta; Z_i)\) and the \(Z_i\) are i.i.d., \begin{equation} \operatorname{Cov}\left(\hat{g}_b(\theta)\right) = \frac{1}{b^2}\sum_{i=1}^b \operatorname{Cov}\left(s(\theta; Z_i)\right) = \frac{1}{b} \operatorname{Cov}_{\mathsf{P}}\left(s(\theta; Z)\right). \end{equation}</p>
<p>Because \(g(\theta) = \mathbb{E}[-s(\theta; Z)]\) is deterministic, we have \(\operatorname{Cov}(\xi_b) = \operatorname{Cov}(\hat{g}_b)\).</p>
<p>If the model is well-specified at \(\theta\), then the score has mean zero: \begin{equation} \mathbb{E}_{\mathsf{P}}\left[s(\theta; Z)\right] = \mathbb{E}_{X}\mathbb{E}_{Y \sim p_\theta(\cdot \mid X)}\left[\nabla_\theta \log p_\theta(Y \mid X)\right] = 0, \end{equation} where the inner expectation vanishes by the standard score identity. Hence the covariance equals the second moment: \begin{equation} \operatorname{Cov}_{\mathsf{P}}\left(s(\theta; Z)\right) = \mathbb{E}\left[s(\theta; Z)s(\theta; Z)^\top\right] = \mathcal{I}(\theta). \end{equation}</p>
<p>Therefore, \(\operatorname{Cov}(\xi_b(\theta)) = \mathcal{I}(\theta)/b\).</p>
<p><strong>Critical Assumption</strong>: This result requires the model to be well-specified. In practice, neural networks are almost always misspecified—they approximate complex real-world distributions but don't match them exactly. When misspecified, \(\mathbb{E}[s(\theta; Z)] \neq 0\), and the covariance is not exactly the Fisher information.</p>
<h3>MAP Bridge Paths are Near-Geodesic</h3>
<p>We model the continuous-time limit of stochastic gradient descent as the Stratonovich SDE<sup><a href="#fn4" id="fnref4" aria-label="Footnote 4">4</a></sup> \begin{equation} \mathrm{d}\theta_t = b(\theta_t)\mathrm{d}t + \sqrt{2\varepsilon} \circ \mathrm{d}B^g_t, \end{equation} where \(B^g_t\) is Brownian motion in the metric \(g\), \(b\) is the drift (deterministic gradient term), and \(\varepsilon = \eta/(2b)\) relates learning rate \(\eta\) and batch size \(b\) to the diffusion strength.</p>
<div class="example-box">
<div class="example-box-title">MAP Bridge Paths are Near-Geodesic</div>
<div class="example-box-prompt">
Let \((\Theta, g)\) be a complete Riemannian manifold with metric \(g\) (the Fisher metric). Consider the SDE above with endpoints \(\theta(0) = \theta_0\) and \(\theta(T) = \theta_T\). Let \(\mathbb{P}^{\varepsilon,T}_{\theta_0,\theta_T}\) denote the bridge measure conditioned on these endpoints.

<strong>A. Driftless Case (\(b \equiv 0\))</strong>: As \(\varepsilon \to 0\), the bridge paths concentrate (in the sense of large deviations) on the constant-speed minimizing geodesic connecting \(\theta_0\) to \(\theta_T\).

<strong>B. With Drift</strong>: As \(T \to 0\) (short-time regime), any sequence of most-probable (MAP) paths \(\theta^*_T\) satisfies
\begin{equation}
    \sup_{t \in [0,T]} d_g(\theta^*_T(t), \gamma(t)) = O(T),
\end{equation}
where \(\gamma\) is the constant-speed \(g\)-geodesic connecting the endpoints.
</div>
</div>
<h3>Proof Sketch</h3>
<p>The proof uses Freidlin-Wentzell large deviations theory<sup><a href="#fn5" id="fnref5" aria-label="Footnote 5">5</a></sup> for diffusions on manifolds. The Onsager-Machlup (OM) functional<sup><a href="#fn6" id="fnref6" aria-label="Footnote 6">6</a></sup> for paths is \begin{equation} \mathcal{I}_T[\theta] = \frac{1}{4}\int_0^T \|\dot{\theta}_t - b(\theta_t)\|^2_{g(\theta_t)} \mathrm{d}t. \end{equation}</p>
<p>For the driftless case, minimizers satisfy the Euler-Lagrange equation, which is precisely the geodesic equation \(\frac{\mathrm{D}}{\mathrm{d}t}\dot{\gamma}(t) = 0\).</p>
<p>For the case with drift, the proof uses \(\Gamma\)-convergence<sup><a href="#fn7" id="fnref7" aria-label="Footnote 7">7</a></sup> as \(T \to 0\). After time rescaling \(s = t/T\), the functional becomes \begin{equation} \hat{\mathcal{I}}_T^b[\theta] = \frac{1}{4} \int_0^1 \norm{\vartheta^\prime}_g^2 \mathrm{d}s - \frac{T}{2} \int_0^1 \langle \vartheta^\prime, b(\vartheta) \rangle_g \mathrm{d}s + \frac{T^2}{4} \int_0^1 \norm{b(\vartheta)}^2_g \mathrm ds. \end{equation}</p>
<p>As \(T \to 0\), this \(\Gamma\)-converges to the pure geodesic energy \(\frac{1}{4}\int_0^1 \norm{\vartheta^\prime(s)}^2_{g(\vartheta(s))} \mathrm ds\), showing that the geodesic term dominates in the short-time limit.</p>
<p>The \(O(T)\) bound follows from second-variation analysis: in a normal convex neighborhood, the index form is strictly positive, giving a quadratic lower bound on the energy difference from the geodesic. The drift terms contribute only linear corrections, leading to the \(O(T)\) path closeness.</p>
<p><strong>Critical Assumption</strong>: Part B only holds as \(T \to 0\). The \(O(T)\) bound means the deviation grows linearly with time. For long training runs (many epochs), this bound doesn't control the deviation—the paths can drift arbitrarily far from geodesics.</p>
<h2>Metric Alignment in Overparameterized Networks</h2>
<p>The third part explains why different optimizers produce similar paths: their metrics conformally align with the Fisher metric at large width. In overparameterized networks, the Fisher information matrix becomes approximately isotropic, and many optimizers' metrics also become isotropic. Conformally equivalent metrics share the same geodesics.</p>
<div class="example-box">
<div class="example-box-title">Metric Alignment in the NTK Regime</div>
<div class="example-box-prompt">
Consider a neural network with \(p\) parameters in the NTK/lazy training regime, where the network remains close to its random initialization. Let \(\mathcal{I}(\theta)\) be the Fisher information matrix and \(G_{\mathrm{opt}}(\theta)\) be the effective metric induced by an optimizer's preconditioner.

Then:
\begin{equation}
    \norm{G_{\mathrm{opt}}(\theta) - c(\theta) \mathcal{I}(\theta)}_{\mathrm{op}} = O_{\mathbb{P}}(p^{-1/2}),
\end{equation}
where \(c(\theta) = \mathrm{Tr}(G_{\mathrm{opt}})/\mathrm{Tr}(\mathcal{I})\) is a normalization constant, and the bound holds uniformly over finite training horizons.

This applies to:
<ul><li>SGD with \(G_{\mathrm{opt}} = I\)</li><li>Adam with \(G_{\mathrm{opt}} = \mathrm{diag}(v_t)\) where \(v_t\) tracks gradient second moments</li></ul>
</div>
</div>
<h3>Proof Sketch</h3>
<p>At random initialization with NTK scaling<sup><a href="#fn8" id="fnref8" aria-label="Footnote 8">8</a></sup>, the Fisher matrix is approximately isotropic: \(\mathcal{I}(\theta_0) = \alpha I_p + R\) where \(\norm{R}_{\mathrm{op}} = O_{\mathbb{P}}(p^{-1/2})\). This follows from coordinate independence of gradients at initialization (tensor program limit<sup><a href="#fn9" id="fnref9" aria-label="Footnote 9">9</a></sup>) and matrix concentration for sums of rank-one outer products.</p>
<p>In the NTK regime, the Jacobian (hence Fisher) remains frozen near its initialization value. For SGD, \(G = I\) is already isotropic. For Adam, coordinate-wise gradient variances \(v_i\) concentrate around their common value \(\alpha\) with uniform deviation \(O_{\mathbb{P}}(p^{-1/2})\), giving \(\mathrm{diag}(v) \approx \alpha I + O(p^{-1/2})\).</p>
<p>By conformal equivalence, since \(G_{\mathrm{SGD}}\), \(G_{\mathrm{Adam}}\), and \(\mathcal{I}\) are all conformally aligned to within \(O(p^{-1/2})\), their geodesics converge as \(p \to \infty\).</p>
<p><strong>Critical Assumption</strong>: This proof only establishes alignment in the NTK regime where training is approximately linear and the Fisher metric remains constant. In the feature-learning regime where representations evolve, the metric changes significantly, and alignment may not hold.</p>
<h2>Why the Theory Fails Empirically</h2>
<p>Empirically, training paths are not near-geodesic in Fisher information space. The theory fails because of assumptions that don't hold in practice.</p>
<h3>The Short-Time Assumption</h3>
<p>Theorem 2 part B only proves \(O(T)\) deviation as \(T \to 0\). For neural network training, we have:</p>
<ul><li>Training happens over long time horizons (many epochs, potentially thousands of steps)</li><li>The \(O(T)\) bound means errors accumulate linearly with time</li><li>Over a full training run, paths can drift arbitrarily far from geodesics</li></ul>
<p>The \(\Gamma\)-convergence argument only controls behavior in the short-time limit. There's no mechanism to prevent long-term drift away from geodesic paths. In fact, the drift term \(b(\theta_t)\) in the SDE can systematically push paths away from geodesics over time.</p>
<h3>The Well-Specified Model Assumption</h3>
<p>The Fisher-covariant noise result requires the model to be well-specified: \(Y \vert X \sim p_\theta(\cdot \vert X)\) under the data distribution. In practice:</p>
<ul><li>Neural networks are almost always misspecified—they approximate but don't match the true data distribution</li><li>When misspecified, \(\mathbb{E}[s(\theta; Z)] \neq 0\) (the score has non-zero mean)</li><li>The gradient noise covariance is \(\operatorname{Cov}(\xi_b) = \frac{1}{b}\mathcal{I}(\theta) + \frac{1}{b}\mathbb{E}[s]\mathbb{E}[s]^\top\), not just \(\frac{1}{b}\mathcal{I}(\theta)\)</li><li>The additional bias term \(\mathbb{E}[s]\mathbb{E}[s]^\top\) breaks the Fisher-covariant structure</li></ul>
<p>This misspecification error accumulates over training, causing paths to deviate from the Fisher geometry.</p>
<h3>The NTK Regime Assumption</h3>
<p>The metric alignment result only holds in the NTK/lazy training regime. However:</p>
<ul><li>Modern neural networks operate in the feature-learning regime where representations evolve significantly</li><li>The Fisher metric changes substantially during training, not just near initialization</li><li>Metric alignment at initialization doesn't guarantee alignment throughout training</li></ul>
<p>Even if metrics align initially, they can diverge as the network learns features and the Fisher information evolves.</p>
<h2>Conclusion</h2>
<p>This whole project is more or less a lesson in empirically verifying predictions before diving into developing the theory, but in any case, it was still an interesting idea.</p>
<h2>References</h2>
<ol><li><a href="https://link.springer.com/10.1007/978-4-431-55978-8">Information Geometry and Its Applications</a> - Amari (2016)</li><li><a href="https://proceedings.neurips.cc/paper/1996/hash/39e4973ba3321b80f37d9b55f63ed8b8-Abstract.html">Neural Learning in Structured Parameter Spaces</a> - Amari (1996)</li><li><a href="https://www.springer.com/gp/book/9780817634902">Riemannian Geometry</a> - do Carmo (1992)</li><li><a href="https://link.springer.com/10.1007/978-3-642-03311-7">Large Deviations Techniques and Applications</a> - Dembo & Zeitouni (2010)</li><li><a href="https://link.springer.com/10.1007/978-3-642-25847-3">Random Perturbations of Dynamical Systems</a> - Freidlin & Wentzell (2012)</li><li><a href="https://doi.org/10.1007/BF01288561">Brownian bridges on Riemannian manifolds</a> - Hsu (1990)</li><li><a href="https://arxiv.org/abs/1806.07572">Neural Tangent Kernel</a> - Jacot, Gabriel & Hongler (2020)</li><li><a href="https://arxiv.org/abs/1812.07956">On Lazy Training in Differentiable Programming</a> - Chizat, Oyallon & Bach (2020)</li><li><a href="https://arxiv.org/abs/1902.06720">Wide Neural Networks of Any Depth Evolve as Linear Models</a> - Lee et al. (2020)</li><li><a href="https://arxiv.org/abs/2203.03466">Tensor Programs V</a> - Yang et al. (2022)</li><li><a href="https://arxiv.org/abs/1806.01316">Universal Statistics of Fisher Information in Deep Neural Networks</a> - Karakida, Akaho & Amari (2019)</li><li><a href="https://arxiv.org/abs/1704.04289">Stochastic Gradient Descent as Approximate Bayesian Inference</a> - Mandt & Hoffman</li><li><a href="https://arxiv.org/abs/1511.06251">Stochastic Modified Equations</a> - Li & Tai</li><li><a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a> - Kingma & Ba (2017)</li></ol>
<div class="footnotes">
<ol>
<li id="fn1">The Fisher information matrix measures the amount of information that an observable random variable carries about an unknown parameter. In machine learning, it quantifies how sensitive the model's predictions are to parameter changes. <a href="#fnref1" class="footnote-backref" aria-label="Back to reference">↩</a></li>
<li id="fn2">The covariant derivative generalizes the notion of directional derivative to curved spaces. The Levi-Civita connection is the unique torsion-free, metric-compatible connection on a Riemannian manifold. <a href="#fnref2" class="footnote-backref" aria-label="Back to reference">↩</a></li>
<li id="fn3">Large deviations theory provides asymptotic estimates for probabilities of rare events. It's particularly useful for understanding the concentration behavior of stochastic processes. <a href="#fnref3" class="footnote-backref" aria-label="Back to reference">↩</a></li>
<li id="fn4">The Stratonovich interpretation of stochastic differential equations is often preferred in physics and geometry because it obeys the chain rule, making it coordinate-independent. It's denoted by the \(\circ\) symbol. <a href="#fnref4" class="footnote-backref" aria-label="Back to reference">↩</a></li>
<li id="fn5">Freidlin-Wentzell theory extends large deviations principles to stochastic differential equations, characterizing the most probable paths of diffusions. <a href="#fnref5" class="footnote-backref" aria-label="Back to reference">↩</a></li>
<li id="fn6">The Onsager-Machlup functional gives the probability density of paths for a diffusion process. Minimizing it yields the most probable path (MAP path). <a href="#fnref6" class="footnote-backref" aria-label="Back to reference">↩</a></li>
<li id="fn7">{\Gamma}-convergence is a notion of convergence for functionals that ensures minimizers converge. It's particularly useful for studying asymptotic behavior as parameters go to zero. <a href="#fnref7" class="footnote-backref" aria-label="Back to reference">↩</a></li>
<li id="fn8">The Neural Tangent Kernel (NTK) regime, also called "lazy training," occurs when networks are so wide that they remain close to initialization during training, making the dynamics approximately linear. <a href="#fnref8" class="footnote-backref" aria-label="Back to reference">↩</a></li>
<li id="fn9">Tensor programs provide a mathematical framework for analyzing infinite-width neural networks, showing that gradients and activations converge to Gaussian processes in the limit. <a href="#fnref9" class="footnote-backref" aria-label="Back to reference">↩</a></li>
</ol></div>
    </main>

    <footer class="footer">
        <div class="last-updated">
            <p>Compiled Nov 16, 2025 at 01:35 | <a href="../src/fisher_tr_dyn.mdtx" target="_blank">Source</a></p>
        </div>
    </footer>

    <!-- Table of Contents Generator - loads before MathJax renders -->
    <script src="../toc-generator.js"></script>

    <!-- Footnote Sidebar - loads before MathJax renders -->
    <script src="../footnote-sidebar.js"></script>
</body>
</html>