<!DOCTYPE html>
<html lang="en">

<head>
    \(\require{physics}\)
    <meta charset="UTF-8">
    <link rel="icon" href="../../favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hopfield Networks</title>
    <style>
        :root {
            --primary-color: #4a4a4a;
            --link-color: #6B46C1;
            --background-color: #fffdf92e;
            --hover-color: #f0f0f0;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Arial, sans-serif;
            line-height: 1.6;
            color: var(--primary-color);
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem;
            background: var(--background-color);
        }

        header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 1rem;
            padding-bottom: .5rem;
            border-bottom: 1px solid #eee;
        }

        .logo {
            font-size: 1.5rem;
            font-weight: bold;
        }

        nav a {
            margin-left: 1.5rem;
            text-decoration: none;
            color: var(--primary-color);
            transition: color 0.2s;
        }

        nav a:hover {
            color: var(--primary-color);
            text-decoration: underline;
        }

        .intro {
            margin-bottom: 1rem;
        }

        .date {
            font-size: 0.9rem;
            color: #666;
            margin-bottom: 2rem;
        }

        h1 {
            font-size: 2rem;
        }

        a {
            color: var(--primary-color);
            text-decoration: underline;
        }

        a:hover {
            color: var(--link-color);
            text-decoration: underline;
        }

        .example-box {
            background-color: #f4f4f4;
            border-left: 4px solid #6a69a0;
            border-radius: 5px 10px 10px 5px;
            padding: 15px;
            margin: 15px 0;
            color: #4a4a4a;
        }

        .example-box-title {
            display: block;
            margin-bottom: 10px;
            color: #6a69a0;
            font-weight: bold;
        }

        .example-box-prompt {
            margin: 10px 0;
        }

        .code {
            background-color: white;
            padding: 10px;
            border-radius: 5px;
            font-family: monospace;
        }

        .example-box-completion {
            margin: 10px 0;
        }

        .example-box-description {
            color: #666;
            font-size: 0.8em;
        }
    </style>
</head>

<body>
    <script src="../toc-generator.js"></script>
    <header>
        <div class="logo"><a style="text-decoration: none; color:#4a4a4a" href="/">DR</a></div>
        <nav>
            <a href="/research">Research</a>
            <!-- <a href="/projects">Projects</a> -->
            <a href="/blog">Blog</a>
        </nav>

    </header>
    <main>
        <article class="intro">
            <h1>Hopfield Networks</h1>
            <p class="date">January 19, 2025</p>
            <p>A quick implementation of a Hopfield network in Python.</p>
        </article>
        <section>
            <h2>Introduction</h2>
            <p>Hopfield networks were one of the earliest generations of neural networks and were/are an important tool in understanding associative memory and learning dynamics. First introduced by John Hopfield in 1982, Hopfield networks are a form of recurrent neural network. Contrary to computer memory, where every piece of information is given an address and accessed by its memory address, content-addressable (or associative) memory retreives saved patterns when given incomplete or noisy input. This works by encoding the saved patterns as minima in an energy landscape, and the network naturally converges to the nearest minima when given an input.</p>
        </section>
        <section>
            <h2>Hopfield Networks</h2>
            <h3>Setup</h3>
            <p>A Hopfield network is a fully connected undirected simple graph, where every node is connected to every other node by an edge with some weight. Each node is a binary value (either -1 or +1). \(\pm\)1 is used instead of 0, 1 because it simplifies the dynamics and calculation of the energy of the network.</p>
            <h3>Energy Function</h3>
            <p>The energy function of the network is defined as
                $$
                    E = -\frac{1}{2} \displaystyle\sum_{i,j} w_{ij} s_i s_j
                $$
            where \(w_{ij}\) is the weight between neruons \(i\) and \(j\), and \(s_i\) is the state of neuron \(i\).</p>
            <h3>Dynamics</h3>
            <p>The network is updated either synchonously or asynchronoously (each node one at a time or all nodes at once, respectively). Each neuron's state is computed as a function of the weighted sum of the other neurons:
                $$
                    s_i = \text{sign} \left(\displaystyle\sum_{j} w_{ij}s_j\right)
                $$
            </p>
            <h3>Computing Weights</h3>
            <p>For a given pattern, the weights are computed according to a Hebbian Learning rule. The goal is to strengthen the connection (increase the weight) of nodes that are active at the same time (which is why we use \(\pm 1\)). For a Hopfield network with \(N\) nodes and a pattern \(\xi\), weights are computed as
                $$
                    w_{ij} = \xi_i \xi_j
                $$
            If \(\xi_i\) and \(\xi_j\) are in the same state, their product is postive and the weight is bigger. Intuitively, this translates to each node having a larger impact on the state of the other.</p>
            <p>To memorize multiple patterns \(\{\xi^\mu\}_{\mu=1}^P\), each weight is the sum of the weights for each pattern:
                $$
                    w_{ij} = \frac{1}{P} \displaystyle\sum_{\mu=1}^P \xi_i^\mu \xi_j^\mu
                $$
            </p>
        </section>
        <section>
            <h2>Implementation</h2>
            <h3>Matrix-Vector Notation</h3>
            <p>If we encode the state of the network as an \(N\)-dimensional vector, a synchronous update rule is given by multiplying by a \(N \times N\)-dimensional weight matrix. This weight matrix is given by the sum of the outer product of each pattern with itself:
                $$
                    \mathbf{W} = \frac{1}{P} \displaystyle\sum_{\mu=1}^P \xi^\mu \otimes \xi^\mu = \frac{1}{P} \displaystyle\sum_{\mu=1}^P {\xi^\mu}^\top \xi^\mu
                $$
            To understand why this is has each memorized vector as the stable point of the network dynamics, consider a transition:
            $$
            W \xi = (\xi \xi^\top) \xi = \xi (\xi^\top \xi) \propto \xi
            $$
            since \(\xi^\top \xi\) is a scalar. Then, we simply superimpose the contributions of each memorized vector \(\xi\).
            </p>
            <h3>Python Implementation</h3>
            <pre><code class="python" style="border-radius: 5px">class HopfieldNetwork:
    def __init__(self, patterns):
        self.patterns = patterns
        self.num_neurons = patterns.shape[1]
        self.weights = self.create_weights(patterns)
            
    def create_weights(self, patterns):
        weights = np.zeros((self.num_neurons, self.num_neurons))
        for pattern in patterns:
            pattern = pattern.reshape(-1, 1)
            weights += np.outer(pattern, pattern)
        np.fill_diagonal(weights, 0) # no self-connections
        return weights / patterns.shape[0]
            
    def update_state(self, state):
        net_inputs = self.weights @ state
        state = np.sign(net_inputs)
        return state
    
    def energy(self, state):
        return -0.5 * state @ (self.weights @ state) 

    def recall(self, initial_state, max_iterations=10):
        state = initial_state.copy()
        energy = self.energy(state)
        for _ in range(max_iterations):
            updated_state = self.update_state(state)
            new_energy = self.energy(updated_state)
            if new_energy == energy:
                break
            state = updated_state
            energy = new_energy
        return state</code></pre>
        <h3>Toy Problem</h3>
        <p>As an example, consider a Hopfield network meant to store 128 x 128 px images.Using the above implementation to learn 4 patterns, the network is able to accurately recall the patterns with up to 40% of the pixels flipped. The specific code used is available for download <a href="hopfield_net.py" download>here</a>.</p>
        <div class="example-box">
            <strong class="example-box-title">Hopfield Network Recall Results</strong>
            <div class="example-box-images" style="display: grid; width: 100%;grid-template-columns: repeat(2, 1fr);gap: 10px; justify-content: center;align-items: center;">
                <img src="imgs/10.png" alt="Recall Result 10" style="width: 90%; border-radius: 5px;" />
                <img src="imgs/20.png" alt="Recall Result 20" style="width: 90%; border-radius: 5px;" />
                <img src="imgs/30.png" alt="Recall Result 30" style="width: 90%; border-radius: 5px;" />
                <img src="imgs/40.png" alt="Recall Result 40" style="width: 90%; border-radius: 5px;" />
            </div>
            <small class="example-box-description">Note: This display style and the specific patterns were adapted from <a href="#gh-repo">takyamamoto/Hopfield-Network</a>.</small>
        </div>
        </section>
        <section>
            <h2>Statistical Mechanics and Deep Learning</h2>
            Hopfield networks have a wide variety of applications in statistical physics and deep learning theory.
            <h3>Ising Model</h3>
            <p>The Hopfield network is closely tied to the Ising model in statistical mechanics. In physics, the Ising model is a lattice structure used to describe ferromagnetic materials. In the Hopfield network, the neurons are analogous to spins at each lattice point, the weights represent the coupling strength between spins, and the energy function describes the total energy of the system.</p>
            <div class="example-box">
                <strong class="example-box-title">Hopfield Network Recall Results</strong>
                    <img src="imgs/ising.png" alt="Recall Result 10" style="max-width: 100%; max-height: 400px; border-radius: 5px;" />
                <p>An Ising model, where each lattice point is an electron with either spin up or down.</p>
                <small>Source: <a href="https://personal.math.ubc.ca/~andrewr/research/intro_html/node14.html" target="_blank">The Ising Model and Counting Graphs</a></small>
                </div>
            <h3>Temperature and Simulated Annealing</h3>
            <p>We can also introduce a temperature into the Hopfield network, which allows it to explore many more states and escape local minima. As the temperature decreases, the model settles into a single minima. Simulated annealing is a process by which the model starts at high temperature and "cools down" over time. This prevents the model from gettign stuck in local minima early on and increasing its chances of settling into a global minima (one of the memorized patterns).</p>
            <h3>Memory and Attention Mechanisms</h3>
            <p><a href="https://arxiv.org/abs/2008.02217" target="_blank">Ramsauer et al. (2020)</a> showed that attention mechanisms in transformers can be viewed as a continuous generalization of Hopfield Networks, implying that Transformers use something similar to associative memory. The same way that tokens attend to each other in transformers, patterns "attend" to each other in Hopfield networks, predicting which pattern to converge to, or which token to predict.</p>
        </section>
        <footer>
            <h2>References</h2>
            <ul style="list-style-type: none;">
                <li id="textbook">[1] <a href="https://neuronaldynamics.epfl.ch/online/Ch17.S2.html" target="_blank">Hopfield Network | Neural Dynamics</a></li>
                <li id="yt-video">[2] <a href="https://www.youtube.com/watch?v=piF6D6CQxUw" target="_blank">Hopfield network: How are memories stored in neural networks? [Nobel Prize in Physics 2024] - Layerwise Learning</a></li>
                <li id="gh-repo">[3] <a href="https://github.com/takyamamoto/Hopfield-Network"
                        target="_blank">takyamamoto/Hopfield-Network</a>
                </li>
            </ul>
        </footer>
    </main>
</body>

</html>