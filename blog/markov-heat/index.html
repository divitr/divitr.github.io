<!DOCTYPE html>
<html lang="en">

<head>
    \(\require{physics}\)
    <meta charset="UTF-8">
    <link rel="icon" href="../../favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="../../style.css">
    <link rel="stylesheet" href="../blog.css">
    <script src="../blog_header.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Markov Picture of Heat</title>
</head>

<body>
    <div id="header-placeholder"></div>

    <main class="blog-post">
        <section class="intro">
            <h1>A Markov Picture of Heat</h1>
            <p class="date">Aug. 23, 2025</p>
            <p class="desc">A derivation of the heat equation from a stochastic model of energy exchange</p>
        </section>

<p><h2>Motivation</h2> Fourierâ€™s law and the heat equation are phenomenological; they describe how temperature fields evolve at macroscopic scales. However, statistical mechanics is concerned with deriving macroscopic properties from microscopic interactions and behaviors. Thus, a natural question is whether we can derive these laws from assumptions that are (i) local, (ii) Markov (memoryless), and (iii) energy-conserving. In the below, we go through a derivation of the discrete Laplacian and how it converges to the heat equation in the hydrodynamic limit.</p>
<p><strong>[EDIT]</strong> About halfway through writing this post, I found the KMP paper <em>Heat Flow in an Exactly Solvable Model</em>, which actually introduces this concept. What's derived here is essentially their model, and their work is a far more rigorous and extensive version of what's presented here.</p>
<p><h2>Model</h2> We will consider the 1 dimensional lattice of \(N\) cells with spacing \(a > 0\). Then, our total length is \(L \doteq Na\). Our system's state at a time \(t\) is the energy vector \begin{equation} \mathbf{e}(t) \doteq (e_1(t), e_2(t), \dots, e_N(t)) \in [0,\infty)^N. \end{equation}</p>
<p>We define energy exchange via a Markov jump process. Specifically, we assume that each bond between particle \(i\), \(i+1\) (a bond is defined only between neighbors) independently operates on a Poisson clock with rate \(\lambda\). When the clock "rings", the two energy sites are redistributed uniformly in a way that preserves their sum, i.e. \begin{equation} (e_i, e_{i+1}) \to \left(US, (1-U)S\right) \text{ for } U \sim \operatorname{Unif}[0,1], S \doteq e_i + e_{i+1}. \end{equation} This defines a continuous time Markov jump process on \([0, \infty)^N\) that preserves the total energy \(E \doteq \sum_i e_i\). It is clear that the process is memmoryless, but verifying the other conditions is a bit more involved:</p>
<div class="example-box">
<div class="example-box-title">Heat Exchange is a Markov Jump Process</div>
<div class="example-box-prompt">
<ul><li><strong>Holding time</strong>: the next ring time is the minimum of \(N-1\) independent \(\operatorname{Exp}(\lambda)\) clocks; thus \(\Delta T \sim \operatorname{Exp}(\lambda(N-1))\)</li><li><strong>Conservation of Total Energy</strong>: By construction, our interaction model preserves total energy between neighbors, summing over all bonds gives that total energy of the system is conserved.</li><li><strong>Generator</strong>: the generator \(\mathcal{L}\) encodes the infinitesimal evolution of the process. It is defined as \((\mathcal{L}f)(\mathbf{e}) = \lim_{h \to 0} \frac{\mathbb{E}[f(\mathbf{e}_{t+h}) - f(\mathbf{e}_t) \mid \mathbf{e}_t = \mathbf{e}]}{h}.\) Intuitively, this is just the expectation of the change in \(f\) over an infinitesimal time period \(h\), conditioned on the current state (and normalized by \(h\)). In our case, the probability that bond \(i\) rings in a time period \([t, t+h]\) is approximately \(\lambda h + o(h)\). If that bond rings, the system jumps to a new state (as specified by the transition above) and the expectation of the difference is \(\mathbb{E}_U [f(\mathbf{e}^{(i, U)}) - f(\mathbf{e})]\), and summing over all \(N-1\) bonds gives the generator</li></ul>
\begin{equation}
            (\mathcal{L}f)(\mathbf{e}) = \lambda \sum_{i=1}^{N-1} \left(\mathbb{E}_U[f(\mathbf{e}^{(U,i)})] - f(\mathbf{e})\right).
\end{equation}
</div>
</div>
<p>We will work with Neumann boundary conditions, i.e. that energy exchange only happens between internal bonds and so there are no boundary terms in the generator.</p>
<h2>Continuity Equation</h2>
<h3>Discrete Laplacian</h3>
<p>For \(L\) the generator of our Markov Jump Process, we have that for any observable \(f\) (to be precise, \(f\) must map to the reals, i.e. \(f: [0, \infty)^N \to \mathbb{R}\)), \begin{equation} (\mathcal{L}f)(\mathbf{e}) = \lambda \sum_{i=1}^{N-1} \left(\mathbb{E}_U\left[f(\dots, US, (1-U)S, \dots)\right] - f(\mathbf{e})\right) + \text{(boundary terms)} \end{equation} (this is identical to what we derived above).</p>
<p>Let us consider only the coordinate observable \(f_i(\mathbf{e}) = e_i\). Only the adjacent bonds contribute, so we have that a single update on \((i-1, i)\) sends \begin{equation} e_i \mapsto (1-U)(e_{i-1} + e_i) \implies \mathbb{E}_U [e_i^\prime - e_i] = \frac{e_{i-1} - e_i}{2}. \end{equation} Similarly, an update on \((i, i+1)\) gives \begin{equation} e_i \mapsto U(e_i + e_{i+1}) \implies \mathbb{E}_U [e_i^\prime - e_i] = \frac{e_{i+1} - e_i}{2}. \end{equation}</p>
<p>Plugging this back into our generator, we have that \begin{equation} (\mathcal{L}f_i) (\mathbf{e}) = \frac{\lambda}{2} \left( e_{i-1} + e_{i+1} - 2e_i \right) \end{equation} and if we define \(m_i(t) \doteq \mathbb{E}[e_i(t)]\) and use the Kolmogorov forward equation, we arrive at the discrete diffusion equation: \begin{equation} \frac{d}{dt} m_i(t) = \frac{\lambda}{2} \left( m_{i-1}(t) + m_{i+1}(t) - 2m_i(t) \right). \end{equation} This is the discrete Laplacian acting on the mean energy.</p>
<p><h3>Continuity Equation</h3> The jump rule defines local energy conservation in every bond. Let us define the local microscopic current across the bond \((i, i+1)\) \begin{equation} j_{i, i+1}(t) \doteq \frac{\lambda}{2}(e_i(t) - e_{i+1}(t)). \end{equation}</p>
<p>Now, if we examine the change in \(e_i(t)\) over a small increment in time \(dt\), we can see that</p>
<ul><li>energy flows <em>into</em> site \(i\) from \(i-1\) when bond \((i-1, i)\) rings, and</li><li>energy flows <em>out of</em> site \(i\) to \(i+1\) when bond \((i, i+1)\) rings</li></ul>
<p>Putting these together, we can rewrite the discrete Laplacian: \begin{equation} \frac{d}{dt} e_i(t) = j_{i-1, i}(t) - j_{i, i+1}(t). \end{equation} The above equation is a pretty intuitive picture of the local energy change; the change in local energy is inflow minus outflow.</p>
<p><h2>Continuum Limit</h2> Let's go back to the discrete mean-energy evolution (the discrete Laplacian from above), but introduce a spatial scaling. Let us define the spatial coordinate of cell \(i\) be \(x_i \doteq ia\), recalling that \(a\) is the lattice spacing. Now, we may define a smooth profile \(e^a(x,t) \doteq m_i(t)\), so that as \(a\) gets small, the discrete vector \((m_i)\) becomes samples of a smooth field \(e(x,t)\).</p>
<p>Since \(e(x,t)\) is smooth, we may Taylor expand the neighbors: \begin{equation} m_{i-1}(t) = e^a(x-a, t) = e^a(x,t) - a \partial_x e^a(x,t) + \frac{a^2}{2} \partial_{xx} e^a(x,t) + O(a^3), \end{equation} \begin{equation} m_{i+1}(t) = e^a(x+a, t) = e^a(x,t) + a \partial_x e^a(x,t) + \frac{a^2}{2} \partial_{xx} e^a(x,t) + O(a^3). \end{equation} Substituting into the RHS of the discrete Laplacian and simplifying yields \begin{equation} \partial_t e^a(x,t) = \frac{\lambda}{2} (a^2 \partial_{xx}e^a(x,t) + O(a^3)). \end{equation}</p>
<p>Note that as \(a \to 0\), the RHS goes to 0 as well, and nothing really happens. Thus, we need to compensate in one of two ways: (i) either rescale time so we can see "slow" dynamics, or (ii) increasing the update rate so that microscopic effects add up to an observable macroscopic effect.</p>
<p><h3>Diffusive Time Scaling</h3> First, let's consider what happens if we keep the rate \(\lambda\) fixed, but define a macroscopic time \(t^\prime \doteq \frac{t}{a^2}\). Via the chain rule, we have \(a^2 \partial_t = \partial_{t^\prime}\). Plugging into the discrete Laplacian, we have \begin{equation} \partial_{t^\prime} e^a(x,t^\prime) = \frac{\lambda}{2} \partial_{xx} e^a(x,t^\prime) + O(a). \end{equation} In the limit \(a \to 0\), we recover \begin{equation} \partial_{t^\prime} e^a(x,t^\prime) = D \partial_{xx} e^a(x,t^\prime), \text{ for } D \doteq \frac{\lambda}{2}. \end{equation}</p>
<p><h3>Jump Rate Scaling</h3> Alternatively, we can also fix the time \(t\) fixed, but scale the update rate as \(\lambda = \frac{2D}{a^2}\), where \(D\) is defined the same way as above. Plugging into the discrete Laplacian, we get \begin{equation} \partial_{t} e^a(x,t) = \frac{2D}{a^2}\frac{a^2}{2} \partial_{xx} e^a(x,t) + O(a), \end{equation} which after simplifying and taking the continuum limit yields \begin{equation} \partial_{t} e^a(x,t) = D \partial_{xx} e^a(x,t), \text{ for } D \doteq \frac{\lambda}{2}, \end{equation} same as above.</p>
<p>This tracks pretty well with our intuition of random walks: each jump moves energy by distance \(a\), so to get macroscopic diffusion, we need \(O(1/a^2)\) jumps per unit time (since variance grows like \(\text{step}^2 \times \text{jumps}\)).</p>
<p>Whether we choose to scale the jump rate or the time, we arrive at the same continuum PDE, where the diffusion constant \(D\) is set by the microscopic parameters \(\lambda, a\).</p>
<p><h3>Fourier's Law</h3> If we write energy as proportional to temperature (\(e = cT\)) then \begin{equation} \partial_t T(x,t) = \frac{D}{c} \partial_{xx} T(x,t), \end{equation} and Fourier's law \(J = - \kappa \partial_x T\) appears with \(\kappa = cD\).</p>
<figure><img src="heat_diffusion.gif" alt="1-D diffusion gif" style="width:100%"><figcaption>1-D heat diffussion governed by the equation derived above.</figcaption></figure>
    </main>

    <footer class="footer">
        <div class="last-updated">
            <p>Compiled Aug 23, 2025 at 20:50 | <a href="../src/markov-heat.mdtx" target="_blank">Source</a></p>
        </div>
    </footer>

    <script>
        // Debug header loading
        console.log('Blog post loaded, checking header...');
        console.log('Current path:', window.location.pathname);
        console.log('Header script loaded:', typeof loadBlogHeader !== 'undefined');
        
        setTimeout(() => {
            const header = document.getElementById('header-placeholder');
            console.log('Header placeholder:', header);
            console.log('Header content:', header.innerHTML);
        }, 1000);
    </script>
    
    <!-- Table of Contents Generator -->
    <script src="../toc-generator.js"></script>
</body>
</html>