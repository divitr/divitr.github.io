<!DOCTYPE html>
<html lang="en">

<head>
    \(\require{physics}\)
    <meta charset="UTF-8">
    <link rel="icon" href="../../favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Training Regimes</title>
    <style>
        :root {
            --primary-color: #4a4a4a;
            --link-color: #6B46C1;
            --background-color: #fffdf92e;
            --hover-color: #f0f0f0;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Arial, sans-serif;
            line-height: 1.6;
            color: var(--primary-color);
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem;
            background: var(--background-color);
        }

        header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 1rem;
            padding-bottom: .5rem;
            border-bottom: 1px solid #eee;
        }

        .logo {
            font-size: 1.5rem;
            font-weight: bold;
        }

        nav a {
            margin-left: 1.5rem;
            text-decoration: none;
            color: var(--primary-color);
            transition: color 0.2s;
        }

        nav a:hover {
            color: var(--primary-color);
            text-decoration: underline;
        }

        .intro {
            margin-bottom: 1rem;
        }

        .date {
            font-size: 0.9rem;
            color: #666;
            margin-bottom: 2rem;
        }

        h1 {
            font-size: 2rem;
        }

        a {
            color: var(--primary-color);
            text-decoration: underline;
        }

        a:hover {
            color: var(--link-color);
            text-decoration: underline;
        }

        .example-box {
            background-color: #f4f4f4;
            border-left: 4px solid #6a69a0;
            border-radius: 5px 10px 10px 5px;
            padding: 15px;
            margin: 15px 0;
            color: #4a4a4a;
        }

        .example-box-title {
            display: block;
            margin-bottom: 10px;
            color: #6a69a0;
            font-weight: bold;
        }

        .example-box-prompt {
            margin: 10px 0;
        }

        .code {
            background-color: white;
            padding: 10px;
            border-radius: 5px;
            font-family: monospace;
        }

        .example-box-completion {
            margin: 10px 0;
        }

        .example-box-description {
            color: #666;
            font-size: 0.8em;
        }
    </style>
</head>

<body>
    <script src="../toc-generator.js"></script>
    <header>
        <div class="logo"><a style="text-decoration: none; color:#4a4a4a" href="/">DR</a></div>
        <nav>
            <a href="/research">Research</a>
            <!-- <a href="/projects">Projects</a> -->
            <a href="/blog">Blog</a>
        </nav>
       
    </header>
    <main>
        <article class="intro">
            <h1>Richness in Training Regimes</h1>
            <p class="date">Dec 26, 2024</p>
            <p>Some notes on <a href="https://arxiv.org/abs/2404.19719">[arXiv:2404.19719]</a>.</p>
        </article>
        <section>
            <h2>Introduction</h2>
            <p>Training a neural net can be broken down into two processes: feedforward inference and backpropogating update. When training, we want to ensure that these processes are well-behaved i.e. feedforward outputs should move in the right direction in finite time and backpropagation should update the hidden representations that allow optimization to proceed stably (without stalling or exploding). To control these processes, we observe a single degree of freedom: richness, defined as the size of the updates to the hidden representations.</p>
            <div class="example-box">
                <strong class="example-box-title">The Richness Scale</strong>
                <img style="width: 100%; border-radius: 5px" src="imgs/richness_scale.png" />
                <small class="example-box-description">Source: [arXiv:2404.19719].</small>
            </div>
        </section>
        <section>
            <h2>Notation</h2>
            <p><ul>
                <li>\(W^{(i,j)}_{\ell}\) refers to the \(i,j\) component of the \(\ell\)'th weight matrix</li>
                <li>\({M}\) is "aligned" with \(\hat{v}\) if \(\norm{{M}\hat{v}}\) typically dominates \(\norm{{M}\hat{u}}\) for some random isotropic unit vector \(\hat{u}\)</li>
                <li>Asympotic Notation<ul>
                    <li>\(a \sim b \equiv a = \Theta(b)\)</li>
                    <li>\(a \underset{\sim}{>} b \equiv a = \Omega(b)\)</li>
                    <li>\(a \underset{\sim}{<} b \equiv a=\mathcal{O}(b)\)</li>
                </ul></li>
                <li>Einstein summation convention</li>
            </ul></p>
        </section>
        <section>
            <h2>Roadmap</h2>
            <p>All analyses are conducted on a 3-layer linear model. As width grows, controlling the size of the gradient relative to the size of the feedforward representation becomes important, so weight matrices are factored into a scalar coefficient \(g\) and a trainable part \({W}\).</p>
            <div class="example-box">
                <strong class="example-box-title">Toy Model Details</strong>
                <p>Train the model
                    $$h_3 \overset{\mathrm{def}}{=}g_3W_3g_2W_2g_1W_1x$$
                    by gradient descent with constant learning rate on loss function \(\mathcal{L}(y, h_3(x))\).
                </p>
                <p>\(W_\ell^{(i,j)} \sim \mathcal{N}(0, \sigma^2)\), and each \(W_\ell\) is paired to a fixed \(g_\ell\). Increasing \(g_\ell\) while keeping \(g_\ell\sigma_\ell\) fixed increases the size of the gradient recieved by \(W_\ell\) while keeping the size of the feedforward signal the same.</p>
                <p>Some more definitions:
                    $$\begin{equation}
                    h_\ell(x) \overset{\mathrm{def}}{=} g_\ell W_\ell h_{\ell-1}(x) \text{ with } h_0(x) = x
                    \quad\quad\quad \text{(Eqn. 1)}\end{equation}$$
                    $$h_\ell \overset{\mathrm{def}}{=} h_\ell(x)$$
                    $$n_\ell \overset{\mathrm{def}}{=} \text{dim }h_\ell$$
                </p>
                <p>Wide network limit: \(n \sim n_1 \sim n_2 \gg n_0 \sim n_3 \sim 1\) (\(n_0, n_3\) are defined by the learning task)</p>
                <small class="example-box-description">Source: [arXiv:2404.19719].</small>
            </div>
            After updating weights by some \(\Delta W_\ell\), the hidden representations are given by
            $$h_\ell + \Delta h_\ell = g_\ell((W_\ell + \Delta W_\ell)(h_{\ell - 1} + \Delta h_{\ell - 1}))$$ Expanding and subtracting the original representation shows that the representation update is 
            $$\Delta h_\ell = g_\ell \underbrace{\Delta W_\ell h_{\ell-1}}_{\text{layer}}
            + g_\ell \underbrace{W_\ell \Delta h_{\ell-1}}_{\text{passthrough}}
            + g_\ell \underbrace{\Delta W_\ell \Delta h_{\ell-1}}_{\text{interaction}} \quad\quad\quad\text{(Eqn. 2)}$$
            In the above, the layer contribution is caused by the update to the current layer's weights, the passthrough contribution is caused by the update to the previous representation passing through new weights, and the interaction contribution is the interaction between the layer update and the previous representation update.
            <div class="example-box">
                <strong class="example-box-title">Criteria for Well-Behaved Training</strong>
                <ol>
                    <strong><li>Nontriviality Criterion (NTC)
                        $$\norm{\Delta h_3} \sim 1$$
                    </li></strong><p>The loss decreases at a width-independent rate.</p>
                    <strong>
                        <li>Useful Update Criterion (UUC)
                            $$\abs{\pdv{\mathcal{L}}{x}^{\top} \Delta h_\ell} \sim 1 \quad \text{ for } \ell \geq 1$$
                        </li>
                    </strong>
                    <p>Each update should contribute to minimizing the loss.</p>
                    <strong>
                        <li>Maximality Criterion (MAX)*
                            $$\norm{g_\ell\Delta W_\ell h_{\ell - 1}} \sim \norm{\Delta h_\ell}$$
                        </li>
                    </strong>
                    <p>A layer's weight update should contribute non-negligibly to the following representation
                    update (i.e. the layer contribution should not be dominated).</p>
                </ol>
                <p>*Not strictly necessary for stable training, but is useful as it prevents "frozen" layers.</p>
                <small class="example-box-description">Source: [arXiv:2404.19719].</small>
            </div>
            <div class="example-box">
                <strong class="example-box-title">Neural Net with NTC, UUC, and MAX Constraints</strong>
                <img style="width: 100%; border-radius: 5px" src="imgs/roadmap_diagram.png" />
                <p>Note that the NTC is a single constraint that applies only to the last layer, the UUC provides one constraint per layer, and the MAX provides one constraint per layer (trivially satsified in the read-in layer).</p>
                <small class="example-box-description">Source: [arXiv:2404.19719].</small>
            </div>
            Counting constraints, the NTC, UUC, and MAX provide 6 constraints. Choosing \(g_\ell, \sigma_\ell\) provides 6 degrees of freedom, and choosing \(\norm{h_\ell}\) bumps us up to 9. Fixing two of the remaining 3 degrees of freedom (after satsifying the 6 constriants) by setting the initial \(\norm{h_1}, \norm{h_2}\) so that the activations are \(\Theta(1)\). Then, we are left with a single degree of freedom.
        </section>
        <section>
            <h2>Deriving the Richness Scale</h2>
            <h3>Derivation of the Richness Scale</h3>
            <p>Using the feedforward equations, enforce \(\norm{h_\ell}^2 \sim n_\ell\) for \(\ell = 1, 2\) (assuming the input satsifies \(\norm{h_0}^2 \sim n_0\)). Taking the square vector norm of \(\text{Eqn. 2}\) shows
                $$g_\ell^2\sigma_\ell^2n_{\ell - 1} \overset{!}{\sim} 1 \text{ for } \ell = 1, 2 \text{ and } g_3^2\sigma_3^2n_2 \underset{\sim}{<} 1 \quad\quad\quad \text{(Eqn. 3)}$$
            </p>
            <p>In the first backward pass, we find that gradient descent causes the weights to align with their inputs:
                $$\Delta W_\ell = -g_\ell \pdv{L}{h_\ell} \otimes h_{\ell - 1} \quad\quad\quad \text{(Eqn. 4)}$$ 
            </p>
            <p>From \(\text{Eqn. 1}\), we derive a recursive expression of the derivative of the loss w.r.t. the representations:
                $$\pdv{\mathcal{L}}{h_{\ell - 1}} = g_\ell W_\ell^\top \pdv{\mathcal{L}}{h_\ell} \implies \norm{\pdv{\mathcal{L}}{h_{\ell - 1}}}^2 \sim g_\ell^2 \sigma_\ell^2 n_{\ell - 1} \norm{\pdv{\mathcal{L}}{h_\ell}}^2 \quad\quad\quad \text{(Eqn. 5)}$$ where "the norm relation follows from applying the central-limit-style argument and using the fact
                that the factors in the right hand side are uncorrelated".
            </p>
            <p>Now, using \(\text{Eqn. 5}\), we can revise \(\text{Eqn. 2}\):
                $$\Delta h_\ell = \underbrace{-g_\ell^2\norm{h_{\ell - 1}}^2 \pdv{\mathcal{L}}{h_\ell}}_{\text{layer}} \underbrace{-g^2_\ell g^2_{\ell -1} \norm{h_{\ell - 2}}^2 W_\ell W_\ell^{\top}\pdv{\mathcal{L}}{h_\ell}}_{\text{passthrough (layer term)}} + \underbrace{\cdots}_{\text{passthrough (other terms)}} + \underbrace{\cdots}_{\text{interaction}}$$
                $$\approx -g_\ell^2 \Big(\norm{h_{\ell - 1}}^2 + g^2_{\ell - 1}\norm{h_{\ell - 2}}^2 W_\ell W_\ell^{\top}\Big) \pdv{\mathcal{L}}{h_\ell} \quad\quad\quad \text{(Eqn. 6)}$$
            </p>
            <p>Some observations about the above: the first term in \(\text{Eqn. 6}\) is the layer contribution an the second term is the non-negligible part of the passthrough contribution. Using the MAX, the first term cannot be dominated by the second. This means that \(\Delta h_\ell\) has a large contribution in the direction of \(\pdv{\mathcal{L}}{h_\ell}\) i.e. the representation update is aligned with the loss gradient w.r.t. the representation. Using this alignment, we can express the UUC in another way:
                $$\norm{\pdv{\mathcal{L}}{h_\ell}}\norm{\Delta h_\ell} \overset{!}{\sim} 1 \quad \text{for} \quad \ell \geq 1 \quad\quad\quad \text{(Eqn. 7)}$$
            Applying this new form of the UUC to \(\text{Eqn. 5}\) gives a constraint relation:
                $$\norm{\Delta h_\ell}^2 \overset{!}{\sim} g_\ell^2 \sigma^2_\ell n_{\ell - 1} \norm{\Delta h_{\ell -1}}^2 \quad \text{for} \quad \ell \geq 2 \quad\quad\quad \text{(Eqn. 8)}$$
            Since the feedforward constraint (\(\text{Eqn. 3}\)) also applies to layer 2, we find that 
            $$\norm{\Delta h_1} \sim \norm{\Delta h_2} \overset{\text{def}}{=} \norm{\Delta h} \quad\quad\quad \text{(Eqn. 9)}$$
            Thus, the wide hidden representations all evolve at the same rate.
            </p>
            <p>Now, applying the UUC as expressed in \(\text{Eqn. 7}\) to \(\text{Eqn. 6}\) directly gives
                $$1 \overset{!}{\sim} \pdv{\mathcal{L}}{h_\ell}^{\top} \Delta h_\ell \sim g_\ell^2\Bigg(\norm{h_{\ell - 1}}^2 \norm{\pdv{\mathcal{L}}{h_\ell}}^2 + g^2_{\ell -1}\norm{h_{\ell -2}}^2 \norm{W_\ell^{\top} \pdv{L}{h_\ell}}^2\Bigg)$$
                $$\sim \frac{g^2_\ell}{\norm{\Delta h_\ell}^2} \Big(\norm{h_{\ell - 1}}^2 + g_{\ell - 1}^2 \norm{h_{\ell - 2}}^2\sigma_\ell^2 n_{\ell - 1}\Big) \quad\quad\quad \text{(Eqn. 10)}$$
            where the passthrough term is zero for \(\ell = 1\).
            </p>
            <p>Now, using the MAX and the NTC,
                $$
                g_\ell \sim \frac{\norm{\Delta h_\ell}}{\sqrt{n_{\ell - 1}}} \sim
                \begin{cases}
                \frac{1}{\sqrt{n_2}}, & \ell = 3, \\
                \frac{\norm{\Delta h_\ell}}{\sqrt{n_{\ell - 1}}}, & \ell \in \{1, 2\}
                \end{cases} \quad\quad\quad \text{(Eqn. 11)}
                $$
            </p>
            <p>Applying \(\text{Eqn. 11}\) to the other constraint relations (\(\text{Eqn. 3}\) and \(\text{Eqn. 8}\)) shows
                $$\sigma_\ell \sim \frac{1}{\norm{\Delta h}} \quad\quad\quad \text{(Eqn. 12)}$$
            </p>
            <p>Finally, examining \(\text{Eqn. 10}\) shows the relative size of the passthrough contributions:
                $$1 \sim \frac{g_\ell^2}{\norm{\Delta h_\ell}^2} \Big(\norm{h_{\ell-1}}^2 + g^2_{\ell-1}\norm{h_{\ell - 2}}^2 \sigma^2_\ell n_{\ell-1}\Big)$$
                $$\sim 1 + g^2_{\ell-1}\norm{h_{\ell-2}}^2\sigma_\ell^2$$
                $$\sim 1 + \norm{h_{\ell-1}}^2\sigma_\ell^2 \quad\quad\quad \text{(Eqn. 13)}$$
            Plugging in values shows that the passthrough contributions for layers 2 and 3 matches the size of the layer contribution.
            </p>
            <div class="example-box">
                <strong class="example-box-title">Passthrough Matches Layer Contribution</strong>
                <img style="width: 100%; border-radius: 5px" src="imgs/passthrough_layer_contribution.png" />
                <small class="example-box-description">Source: [arXiv:2404.19719].</small>
            </div>
            <p>From \(\text{Eqn. 11}\) and \(\text{Eqn. 12}\), all the constraints have determined the scaling of the initial hyperparameters in terms of a single degree of freedom: \(\norm{\Delta h}\).</p>
            <div class="example-box">
                <strong class="example-box-title">Choosing \(g_\ell, \sigma_\ell\) based on \(\norm{h_\ell}\)</strong>
                <img style="width: 100%; border-radius: 5px" src="imgs/table_1.png" />
                <p>Note that \(n_1, n_2\) are abbreviated as \(n\).</p>
                <small class="example-box-description">Source: [arXiv:2404.19719].</small>
            </div>
            <h3>Richness Values</h3>
            <p>From \(\text{Eqn. 3}\), \(\sigma^2_3 \underset{\sim}{<} 1 \implies \norm{\Delta h} \underset{\sim}{>} 1\). To ensure that the representation updates are not too large, we impose a criterion that \(\norm{\Delta h_2} \underset{\sim}{<} \norm{h_2} \sim \sqrt{n}\). These conditions together define a continuous scale of possible richenss of training:
            $$\boxed{\norm{\Delta h} \sim n^r \quad\quad \text{where the richness \(r\) satisfies} \quad\quad 0 \leq r \leq \frac{1}{2}}$$</p>
            </section>
            <section>
                <h2>Understanding the Richness Scale</h2>
                <h3>Weights Update to Align with Their Input</h3>
                <p>Deriving the weight update in \(\text{Eqn. 4}\) using gradient descent with learning rate \(\eta = 1\) shows
                    $$\Delta W_\ell^{(i,j)} = -\pdv{\mathcal{L}}{W_\ell^{(i,j)}} = -\pdv{\mathcal{L}}{h_\ell^{(k)}}\pdv{h_\ell^{(k)}}{W_\ell^{(i,j)}}$$
                    $$= - \pdv{\mathcal{L}}{h_\ell^{(k)}}\Big(\delta^{(i,k)}g_\ell h^{(j)}_{\ell-1}\Big)$$
                    $$= -g_\ell \pdv{\mathcal{L}}{{h_\ell^{(i)}}}h_{\ell-1}^{(j)}$$
                The Kronekcer Delta indicates that \(\pdv{h_\ell}{W_\ell}\) is a sparse tensor and its contraction with \(\pdv{\mathcal{L}}{h_\ell}\) results in a rank-1 update. Since the input \(h_{\ell-1}\) is the only nonzero right singular vector of the update, the update must be aligned.
                </p>
            <h3>Weight Alignment Does Not Magnify Gradients</h3>
            <p>The upper bound on \(\norm{\Delta h}\) prevents the gradients being magnified and destabilizing training. Computing a second backward pass and examining the backpropagation equations shows this:
                $$
                    \pdv{\mathcal{L}}{h_{\ell-1}} = g_\ell (W_\ell + \Delta W_\ell)^{\top} \pdv{\mathcal{L}}{h_\ell}
                $$
            </p>
            <p>To ensure that the gradient is not magnified, we require that the second term does not dominate. From \(\text{Eqn. 5}\), we know the squared norm of the first term is \(g_\ell^2\sigma^2_\ell n_{\ell-1}\norm{\pdv{\mathcal{L}}{h_\ell}}^2\). The squared norm of the second term is \(g_\ell^4 n_{\ell-1} \norm{\pdv{\mathcal{L}}{h_\ell}}^4\). Simplifying the desired inequality shows
                $$
                    \norm{\Delta h}^2 \underset{\sim}{<} \norm{\Delta h_\ell}^2 n_{\ell - 1} \quad \text{ for } \quad \ell \geq 2
                $$</p>
                <p>Clearly for \(\ell = 3\), choosing \(\norm{\Delta h} \underset{\sim}{<} \sqrt{n}\) prevents the updates from magnifying the gradient.</p>
            <h3>Models Train Lazily iff They Are Linearized</h3>
            <p>Some models are well approximated by their first-order Taylor approximation in parameter space:
                $$
                    f(x; \theta_0 + \Delta \theta) = f(x; \theta_0) + \Delta \theta^\top (\nabla_\theta f(x; \theta_0)) + \underbrace{\frac{1}{2}\Delta \theta^\top (\nabla^2_\theta f(x; \theta_0))\Delta \theta + \cdots}_{\text{negligible in the wide + lazy limit}}
                $$
            </p>
            <p>If \(\Delta \theta\) is a gradient descent update, the first-order gradient term scales like \(\Delta \theta^\top (\nabla_\theta f(x; \theta_0)) \sim 1\), while the second-order curvature term scales like \(\nabla \theta^\top (\nabla^2_\theta f(x; \theta_0)) \Delta \theta \sim \frac{\norm{\Delta h}^2}{n}\). Then, as \(n \to \infty\), the curvature vanishes as long as \(\norm{\Delta h} \ll \sqrt{n}\), i.e. as long as we're not in the \(\mu \text{P}\) regime.</p>
            <h3>Small Initial Outputs are Necessary for Representation Learning</h3>
            <p>Since \(\norm{h_3} \sim \frac{1}{\norm{\Delta h}}\), in order to achieve feature learning in the \(n \to \infty\) limit, there must be small outputs at initialization. Conversely, if we desire the initial outputs to be \(\Theta(1)\), we must accept lazy training.</p>
            <h3>Model Rescaling Emulates Training at Any Richness</h3>
            <p>We can also initialize a network in the NTK regime and toggle rich training by rescaling only the final-layer gradient multiplier and global learning rate:
                $$
                    \text{optimize } \quad \mathcal{L}(\gamma^{-1} f^{\text{(NTK)}}(x), y) \quad \text{ with learning rate } \quad \eta = \gamma^2
                $$ <a id="backlink-1" href="#footnote-1">[1]</a>
             However, we can emulate training with richness \(r\) by choosing \(\gamma \overset{!}{=} n^r\). After implementing the global learning rate as global multipliers, we have $$
                \eta = 1 \quad \quad g_\ell = \gamma g_\ell^{\text{(NTK)}} \quad \quad \sigma_\ell = \gamma^{-1}\sigma_\ell^{\text{(NTK)}}
            $$
            </p>
            <p>Multiplying the model rescaling coefficient into \(g_3\), we have \(g_3 = g_3^{\text{(NTK)}}\). From Table 1, setting \(\gamma = \norm{\Delta h}\) allows the rescaled model to achieve training at any richness.</p>

            <p id="footnote-1"><a href="#backlink-1">[1]</a> At the time of writing, \(\gamma\) was interpreted as written in the equations above, but since then the notation has
            changed. Now \(\gamma \overset{\text{def}}{=} \frac{\gamma_0}{\sqrt{n}}\), where \(\gamma_0\) denotes the gamma used in
            the equations above.</p>
            
            <h3>Layerwise Learning Rates Introduce Families of Equivalent Parameterizations</h3>
            <p>To change the size of the update, changing either the learning rate or the size of the gradient signal are equivalent.</p>
            <div class="example-box">
                <strong class="example-box-title">The Richness Transformation and Gauge Freedom</strong>
                <img style="width: 100%; border-radius: 5px" src="imgs/gauge_freedom.png" />
                <p>Recall this uses the old definition of \(\gamma\) (the same one used in the equations above).</p>
                <small class="example-box-description">Source: [arXiv:2404.19719].</small>
            </div>
            </section>
        <footer>
            <h2>References</h2>
            <ul style="list-style-type: none;">
                <li id="paper">[1] <a
                        href="https://arxiv.org/pdf/2404.19719"
                        target="_blank">The lazy (NTK) and rich (\(\mu\)P) regimes: A gentle tutorial - Dhruva Karkada</a>.</li>
            </ul>
        </footer>
    </main>
</body>

</html>